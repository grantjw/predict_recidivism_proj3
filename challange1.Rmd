---
title: "Challanege1"
author: "Grant Kim, Ayan Gupta"
date: '2020 5 4 '
output:
  html_document: default
  pdf_document: default
---
In this project, we use elastic net, random forests, and gradient boosted trees to predict recidivism based on the metric "precision" (We did include other metrics for exploration purposes). All of our conclusions are in the last page and our choice of specific parameters are explained as we build our model.

# Clean dataset to use for all models
```{r}
#Import Dataset 
df <- read.csv("recidivism_data_sample.csv")

# Make dummy variables for different races

df$white = ifelse(df$race == 1, 1, 0)
df$black = ifelse(df$race == 1, 1, 0)
df$hispanic = ifelse(df$race == 1, 1, 0)
df$asian = ifelse(df$race == 1, 1, 0)
df$native_american = ifelse(df$race == 5, 1, 0)
df$other = ifelse(df$race == 6, 1, 0)

```


# Elastic Net Model
```{r}
library(glmnet)

## Create our matrix of predictor variables and a vector for our outcomes, this will help in the setup of the elastic net model as we use this matrix and vector to define our training and test data
x <- model.matrix(recidivate ~ sex + age + juv_fel_count + juv_misd_count + priors_count + charge_degree + charge_name + white + black + hispanic + asian + other, data = df)[, -1]
y <- df$recidivate

## Setting seed before sampling so that we can replicate results from randomizing later
set.seed(123)

## Setting a proportion of 80/20 training to test data 
prop.train = 0.8

## Generate a vector of rows to hold-out from dataset
r <- sample(nrow(x), nrow(x)*prop.train, replace = FALSE)

## Divide the predictors and labels into training and test sets
x.train <- x[r, ]
x.test <- x[-r, ]
y.train <- y[r]
y.test <- y[-r]

## Defining empty data frame to store results
results <- data.frame()
```

We decided to use an Elastic Net model as one of our models to utilize the shrinkage methods because there are a lot of predictors in this dataset and shrinkage methods may give us a more effective model by reducing the coefficients of the less important predictors. So we used elastic net to find the optimal alpha value for our model, where alpha is a weighting term between the two shrinkage methods, Ridge Regression and the LASSO. 

In the for loop below we test 21 different alpha vaules from 0 to 1 by incrementally increasing alpha by 0.05 every iteration. If the value of of alpha is 0 it means we are using all Ridge Regression and no LASSO. If it is 1 we are using all LASSO and no Ridge Regression, so any value less than 0.5 puts more weight on Ridge Regression while any value above 0.5 puts more weight on the LASSO. 

We chose to use logistic regression because we are trying to predict a binary variable with this model.In the elastic net we caculated the precision of each iteration and used the maximum value of precision to select which alpha we would use in this model.

```{r}
for (i in seq(0, 1, 0.05)) {
  cv.results <- cv.glmnet(x = x.train, y = y.train, 
                          family = "binomial",
                          nfolds = 5, alpha = i)
  
  optimal.lambda.hat = cv.results$lambda.1se
  
  y.pred = predict(cv.results, s = optimal.lambda.hat, newx = x.test, type = "response")
  
  y.pred.hat = ifelse(y.pred >= 0.5, 1, 0)
  
  precision.hat = sum(y.pred.hat[y.test == 1])/sum(y.pred.hat)
  
  temp = data.frame(alpha = i, precision.hat = precision.hat)
  results = rbind(results, temp)
}
results

## Finding the maximum value of precision.hat and storing it's correspoding alpha value into alpha.max for future use, this will allow our model to work for any set of test data because we aren't hard coding any alpha value into the model and instead are finding the alpha value that corresponds with the model that maximized precision.
precision.hat.max = max(results[,2])
alpha.max = results[results$precision.hat == precision.hat.max, "alpha"]
alpha.max

## Running optimal model after finding alpha.max
opt.model = cv.glmnet(x = x.train, y = y.train, 
                          family = "binomial",
                          nfolds = 5, alpha = alpha.max)

## Plotting optimal model's log(lambas) vs binomial deviance, to visualize where the minimum lambda value is and where lambda.1se is.
plot(opt.model, main = "log(lambda) vs. binomial deviance")

## We decided our optimal lambda would be the lambda of the simplest model within one standard deviation of the minimum lambda because we went over in class how it is common practice to use lambda.1se because simpler models don't overfit training data as much as more complex models and therefore in theory this lambda is close enough to the minimum lambda to be used in our model while also potentially being better suited to predict on training data than a more complex model would be.
opt.lambda = opt.model$lambda.1se

## Using optimal model to make predictions on test dataset, to see if our model could work well with test data it wasn't previously exposed to
opt.model.pred = predict(opt.model, newx = x.test, s = opt.lambda, type = "response")

## Applying threshold of 0.5 to predictions to make final predictions in someone will recidivate, we used this threshold because we thought it was logical to believe that if the model predicts a person has a greater than 50% chance of recidivating we should predict that that person did recidivate or will recidivate in the future.
y.test.hat = ifelse(opt.model.pred >= 0.5, 1, 0)

## Accuracy = 67.58%
mean(y.test.hat == y.test)

## Precision = 67.66%
sum(y.test.hat[y.test == 1])/sum(y.test.hat)

## Recall = 47.97%
sum(y.test.hat[y.test == 1])/sum(y.test)
```


Our chosen summary metric "precision" yielded 67.67% for our elastic model. We will explore other models. 

# Random Forest Model with Out of Bag(OOB) error estimation 

## Get a cleaned dataset 
```{r}
#Import Dataset 
df <- read.csv("recidivism_data_sample.csv")

# Make dummy variables for different races

df$white = ifelse(df$race == 1, 1, 0)
df$black = ifelse(df$race == 1, 1, 0)
df$hispanic = ifelse(df$race == 1, 1, 0)
df$asian = ifelse(df$race == 1, 1, 0)
df$native_american = ifelse(df$race == 5, 1, 0)
df$other = ifelse(df$race == 6, 1, 0)
#drop id so that we can use . instead of writing all variables when training 
drops <- c ("race","id")
df <- df[ , !(names(df) %in% drops)]

#80/20 split 

# set a seed for future replication
set.seed(1)

#randomly sample .8 of the data and give their row numbers to k
k <- sample(1:nrow(df), round(nrow(df)*.8), replace = FALSE)

#create new train and test datasets using k
df.train <- df[k,]
df.test <- df[-k,]
```

```{r}
library(randomForest)

# Change outcome variable to "factor" 
df.train$recidivate <- factor(df.train$recidivate, labels = c("FALSE", "TRUE"))

# set the number of variables p or m to iterate (We have 13 predictors, we use sqrt(13) = 3.605 +- 4) 
n.vars <- c(1,2,3,4,5,6,7,8,9,10)
#initialize a list to store iterated models
rf.models <- list()
set.seed(207)

# iterate over 1-10 = m = i to get models with different values of m with 500 trees 
# We have already checked 1000 trees is uncessary since error.rate stabilizes 
for (i in 1:length(n.vars)){
  
  rf.models[[i]] <- randomForest(recidivate ~ ., # input all predictors with outcome = recidivate
                                 data = df.train, ntree = 500, #set number of total trees B = 500
                                 mtry = n.vars[i]) # mtry = select 1-10 variables at random from p
  print(i)
  
}

# Find out best model from iterated model to get final model 
# initialize a list to store iterated values from models

n.mods <- length(rf.models)
oob.error <- rep(NA,n.mods)
n.vars <- rep(NA,n.mods)

# We store the error rate at the 500th row = 500th tree for each model i
for (i in 1:10){
  oob.error[i] <- rf.models[[i]]$err.rate[nrow(rf.models[[i]]$err.rate),1] 
  n.vars[i] <- rf.models[[i]]$mtry # store m used for model i
}
```
NOTE: we choose the 500th tree because:

1) As will be proven in the graph below, error rate stabilizes after a couple hundred trees. 

2) If we use minimum error rate, we risk getting a model that had a low error rate due to random chance, as we have already tested. So it is safer to use models that have stabilized completely. 

3) If we use min error rate, we find a lot of zero values for error rate for each model, which means that we cannot make a conclusion which model is based on those zero errors. How can we compare zeros to zeros in choosing a model? 

```{r}
library(ggplot2)
# find at which index the minimum error rate is 
best.mod <- which.min(oob.error)

# find variables used at the third iteration = 3
n.vars[best.mod]


#plot error rate for our best model
#create a dataframe that formats the error rate information for ggplot2
oob.error.data <- data.frame(
  Trees = rep(1:nrow(rf.models[[best.mod]]$err.rate), times =3),
  Type = rep(c("OOB", "FALSE", "TRUE"), each=nrow(rf.models[[best.mod]]$err.rate)),
  Error = c(rf.models[[best.mod]]$err.rate[,"OOB"],
            rf.models[[best.mod]]$err.rate[,"FALSE"],
            rf.models[[best.mod]]$err.rate[,"TRUE"])
)
#Graph oob error rate 
ggplot <- ggplot(data=oob.error.data, aes(x=Trees, y=Error)) + geom_line(aes(color=Type))
ggplot + ggtitle("error rates at each tree") + theme(plot.title = element_text(lineheight=.8, face="bold"))

```


The graph represents error rates at each tree. 
Blue: error rate when classifying recidivate = FALSE
R: error rate when classifying recidivate = TRUE
Green: overall OOB error rate 
-Error rate stabilizes after number of trees past 100
-Increasing number of trees to 5000 did not create any difference, 


```{r}
set.seed(123)
# predict using best model, we use predicted probabilites
rf.pred.prob <- predict(rf.models[[best.mod]], newdata = df.test, type = "prob")

# create a dataframe to compare outcome value vs. predicted value 
ldat <- data.frame(outcome.recid = df.test$recidivate,
                   pred.recid = factor(rf.pred.prob[,1] < rf.pred.prob[,2]))

#change "TRUE" to 1 , "FALSE" to 0 for pred.recid
ldat$pred.recid = ifelse(ldat$pred.recid == TRUE, 1, 0)

#TP
ldat$true_positive_filter = ldat$outcome.recid == 1 & ldat$pred.recid == 1 
TP <- sum(ldat$true_positive_filter == TRUE)

#TN
ldat$true_negative_filter = ldat$outcome.recid == 0 & ldat$pred.recid == 0
TN <- sum(ldat$true_negative_filter == TRUE)

#FP
ldat$false_positive_filter = ldat$outcome.recid == 0 & ldat$pred.recid == 1 
FP <- sum(ldat$false_positive_filter == TRUE)

#FN
ldat$false_negative_filter = ldat$outcome.recid == 1 & ldat$pred.recid == 0 
FN <- sum(ldat$false_negative_filter == TRUE)


#Accuracy
accuracy <- (TP + TN) / (TP + TN + FP + FN) 
print(accuracy)

#Precision
precision <- TP / (TP + FP)
print(precision)

#Recall 
recall <- TP / (TP+FN)
print(recall)

```
Although it is noted in the lecture that when p is large and fraction of relevant variables is small, 
random forests are likely to perform poorly with small m, m = 3 has the best precision. In turn, we choose the default best model with m =3 , trained with 500 trees. 

**Our random forest model yielded 69.50% precision.**


# Gradient Boosted Trees with Cross-Validation

We want to note that we first tried the method given in the class example, where we iterate over different interaction depths with cross-validtion. Different learning rates, number of trees, and feature engineering were tried, but the metrics hardly moved. The different models we tried are shown in the table below with different models 1-6. 

*NOTE: the plotting function kept producing error, so we had to manually create a table. 
```{r}
precision_br <- c(.6694737, 0.6735967, 0.6751055, 0.6694387, 0.6722689, 0.6680244)
optimal_depth_t <- c(4,5,2,3,2,3)
optiaml_trees_t <- c(497,4678,867,6627,7249,590)

smoke <- matrix(c(precision_br, optimal_depth_t, optiaml_trees_t),ncol=6,byrow=TRUE)
colnames(smoke) <- c("Model1","Model2","Model3","Model4","Model5","Model6")
rownames(smoke) <- c("Precision","Optimal Depth","Optimal Trees")
smoke <- as.table(smoke)
smoke
```

Additional explanation: 

Model 1 was trained with 2,000 trees, 0.01 shrinkage, depth 1-32.  

Model 2 was trained with 10,000 trees, with 0.001 shrinkage, depth 1-8.  

Model 3 was trained with 2,000 trees, 0.01 shrinkage, depth 1-8

Model 4 was trained with 10,000 trees, with 0.001 shrinkage, depth 1-8 

Model 5 was trained with 10,000 trees, with .001 shrinkage, depth 1-8

Model 6 was trained with 2,000 trees, with 0.01 shrinkage, depth 1-8

*MODEL 3 and 4 had feature engineering: we binned "juv_fel_count", "juv_misd_count", "priors_count."

*MODEL 5 and 6 had feature engineering: we included only the most relevant predictors: age, priors_count, charge_name

**The code for model 1-6 is shown in the last page for reference, because this is not our final method for gradient boosted trees. We wanted to mention that we tried the method given in class.** 

### Instead, we shift our focus to model selection using "hyperparameter" tuning with using cross-validation. This method allowed us to try other tuning parameters than just number of iterated trees and interaction.depth.


# Gradient Boosted Trees with Hyperparameter tuning & Cross Validation
```{r}
# re-import dataset from pre-cleaned df dataset for a clean start
df.train <- df[k,]
df.test <- df[-k,]

```

```{r}
library(gbm)
# create hyperparameter grid
hyper_grid <- expand.grid(
  n.trees = c(2000,5000), # number of trees / Boosting iterations = B, we aim to overshoot to find optimal B
  shrinkage = c(.01, .005), # learning rate 
  interaction.depth = c(2,3,4,5,6,7), # number of d splits in each tree 
  n.minobsinnode = c(5, 10, 15), # minimum number of observations in the terminal nodes of the trees
  bag.fraction = c(.5, .75, 1), # data sub-sampling 
  optimal_trees = 0,               # a place to dump results
  min_cv_error = 0                     # a place to dump results
)

# total number of combinations = 216
nrow(hyper_grid)

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  bt.models <- gbm(
    formula = recidivate ~ ., distribution = "bernoulli", #set "bernoulli" for classification
    data = df.train,
    n.trees = hyper_grid$n.trees[i],
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i], 
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    cv.folds = 10,                                      #cv.folds implements cv with 10 k-folds
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(bt.models$cv.error)# store optimal tree based on min cv.error
  hyper_grid$min_cv_error[i] <- min(bt.models$cv.error)# store min cv.error for the current model 
  print(i)
}

#sort based on ascending cv.error
hyper_grid$min_cv_error <- sort(hyper_grid$min_cv_error, decreasing = FALSE)

hyper_grid[1:10,] #first ten lowest cv.error parameters 

```


Based on the first ten rows of cv.error, we narrow our search. 
- optimal number of trees does not shoot over 1654, so we stick to training with 2000 trees
- shrinkage rate works for both values, we try with both values again
- number of nodes of 5 was only used, we just use 5 
- bag fraction of .5 was only used, maybe .75 was too large, we try a smaller value 

## Narrowed Hyperparameter search Graident Boosted Trees
```{r}
#narrow the grid search t
hyper_grid_2 <- expand.grid(
  n.trees = c(2000), # number of trees / Boosting iterations = B, we aim to overshoot to find optimal B
  shrinkage = c(.01, .005), # learning rate 
  interaction.depth = c(2,3,4), # number of d splits in each tree 
  n.minobsinnode = c(5), # minimum number of observations in the terminal nodes of the trees
  bag.fraction = c(.5, .6), # data sub-sampling 
  optimal_trees = 0,               # a place to dump results
  min_cv_error = 0                     # a place to dump results
)


for(i in 1:nrow(hyper_grid_2)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  bt.models <- gbm(
    formula = recidivate ~ ., distribution = "bernoulli", #set "bernoulli" for classification
    data = df.train,
    n.trees = hyper_grid$n.trees[i],
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i], 
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    cv.folds = 10,                                      #cv.folds implements cv with 10 k-folds
  )
  
  # add min training error and trees to grid
  hyper_grid_2$optimal_trees[i] <- which.min(bt.models$cv.error)
  hyper_grid_2$min_cv_error[i] <- min(bt.models$cv.error)
  print(i)
}
hyper_grid_2[1:12,]

```

The first 12 rows of the hyperparameter grid tells us that we have similar cv_error with different parameters for shrinkage, depth, and sub-sampling. We will try all parameters from row 1-12 to see which one yields the highest "precision" metric. 

## Predict using grid search using each values from rows, find the best parameter 
Note: the code displayed are parameters from 10th row of Hyper_grid 2, which is our best model for "precision." 
```{r}
set.seed(123)
#train the model using the parameters at row 10 in hypergrid_2 
LA <- gbm(recidivate ~ ., 
                        data = df.train, shrinkage = 0.005,
                        distribution="bernoulli", n.trees = 2000, 
                        interaction.depth = 3,
                        n.minobsinnode = 5,
                        bag.fraction =  .6,
                        cv.folds =10)
#Just to make sure the optimal tree number is still 548 as noted in hyper_grid2, we find which tree has the lowest cv.error in this training model. 
which.min(LA$cv.error)

#plot N.trees vs. CV.error for reference 
plot(x = 1:2000, y = LA$cv.error,
     main = "N.trees vs. CV.error",
     xlab = "Number of trees",
     ylab = "CV.error")

#predict using best trained model
test.dat.preds <- predict(LA, newdata = df.test, 
                          n.trees = 548, type ="response")
#set a threshold 
threshold <- 0.5
#if predicted probability is greater than 0.5 say that the person will recidivate = 1 
y.test.hat <- as.numeric(test.dat.preds > threshold)

Model_8_Hyper2_ACC10 <- mean(df.test$recidivate == y.test.hat) #accuracy
print(Model_8_Hyper2_ACC10)

Model_8_Hyper2_PRC10 <- sum(y.test.hat[df.test$recidivate == 1])/sum(y.test.hat) #precision
print(Model_8_Hyper2_PRC10)

Model_8_Hyper2_RC12 <- sum(y.test.hat[df.test$recidivate == 1])/sum(df.test$recidivate) #recall
print(Model_8_Hyper2_ACC10)

```

The graph tells us that cv.error is minimized with trees past 500, but starts to increase past 1500.

**Best precision was achieved using the 10th row from hyper_grid_2, with "68.55%" precision.**  

NOTE: optimal tree from hyperparameter grid_2 says that optimal tree number is 548. However, the optimal tree in the trained model (using the parameters from row 10) yields a different optimal tree 1221. This is probably due to some randomness in k-folds cross validation yielding different cv.errors for the hyper_grid2 training and training again using the parameters from row 10. We tried both optimal tree numbers 1248 and 548. We choose 548 as the optimal number because optimal tree with 1221 yields 67.36%, a lower precision. 

Although there is randomness involved, the model trained with the parameters from the 10th row consistently yields precision around 68%. Since this model is not our final model decision. We will not calculate a 95 percentile. 

**The parameters used from the 10th row from hyper_grid_2, with: **

**learning rate: 0.005**

**interaction depth: 3**

**Data-subsampling: .6**

**minimum # of obs in terminal nodes: 5**

**optimal trees: 548** 


**The table below shows precision based on different parameters from Hyper_grid2, from row 1 to row 12** 


```{r}
pre <- c(.6743, .6745, .6625, .674, .669, .677, .670, .6775, .675, .685, .669, .680)


tab <- matrix(c(pre),ncol=12,byrow=TRUE)
colnames(tab) <- c("row_1","row_2","row_3","row_4","row_5","row_6","row_7","row_8","row_9","row_10","row_11","row_12")
rownames(tab) <- c("Precision")
tab <- as.table(tab)
tab
```



# Choosen Metric: "Precision" 
In choosing our metric, we deemded that the alogrithm's priority should be to predict "recidivate" for only those who actually recidivates. We hope to reduce "false positives" while increasing "True Positives." In other words, we ask "for what proportion does the algorithm say it will recidivate correctly"? Precision and FPR are seen as more important as they are more related to costs to defendants. The algorithm at least should not put defendants in jail who actually would not recidivate. In contrast, metrics such as FNR and Recall are more related to costs to society, as societies do not want defedants to recidivate in society. Although costs to society is important as well. The cost to defedants is argubly more important in our perspective. 



# Final model decision

Consider the precision for each model: **Elastic Net: "67.67%", Random Forests: "69.50%", Gradient Boosted Trees: "68.55%."** The best performing model in terms of prediction is the Random Forest model. Out of the three models Random Forests performed best on the precision metric. Although we did not calculate an estimated confidence interval for these metrics, Random Forests will still perform better than other models as precision is higher than gradient boosted trees by 1% and higher than elastic nets by 2%. 

Furthermore, Random Forests is a good model to use since it uses bagging, averaging the predictions of 500 trees with a bootstrapped re-sample of the data for each model, allowing well suited for high variance and low-bias procedures. Bagging, reduces variance since it averages the resulting predictions. 

Random forests are also useful because they have OOB error estimation, allowing to estimate test error of a model without the need for cross-validation by using each bagged tree that has 2/3s of the training set and the remaining 1/3s as the out of bag observations. This allows averaging of predicted responses for the ith observation, where the overall OOB classifcation error can be computed for each bagged model. 

Lastly, we dicuss why gradient boosted trees and elastic net is not our model choice. First, the elastic net model is good because it shrinks predictor variables and prevents the model from going overly complex, which may help prevent overfitting at the expense of bias increase. However, the same objective can be reached with random forests by training with a large number of trees, thereby preventing overfitting. In addition, random forests uses bagging automatically while the elastic net does not have bagging in this model. Therefore we would choose the random forest over gradient boosted trees. 

The gradient boosted tree as a model does not lack the predicitive capability of that of random forests. Gradient boosted trees works like the random forests but trees are just grown sequenitally, using trees from previously grown trees, with a slow learning rate such as 0.01 or 0.001. 

Depspite GBT's predictive powerfulness we choose the random forest model over GBT for several reasons. First, the random forest model did perform better on the precision metric than GBT by 1%. Second, there were two optimal number of trees, 548 trees from the hyper_grid 2 training, and 1221 trees from re-training using the parameters from hyper_grid2. In turn, choosing between the two optimal number of trees was a choice we had to make based on insufficient information. The model with 548 trees was chosen because it performed better for precision. Although this is consistent with the text: "because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient," as a machine learning novice, we thought it was a safer decision to go with the random forests rather than making a subjective decision on the optimal number of trees using gradient boosted trees. Third, we played around with the parameters for gradient boosted trees a lot more than random forests, but could not reach the precision rate of the random forest model. If we did a grid search like the gradient boosted trees, the random forest may have performed even better.  

Thus, the random forest model is our last model of choice.


# Check Final model with Pusedo dataset
```{r}
df.pseudo <- read.csv("recidivism_data_pseudo_new.csv")

df.pseudo$white = ifelse(df.pseudo$race == 1, 1, 0)
df.pseudo$black = ifelse(df.pseudo$race == 1, 1, 0)
df.pseudo$hispanic = ifelse(df.pseudo$race == 1, 1, 0)
df.pseudo$asian = ifelse(df.pseudo$race == 1, 1, 0)
df.pseudo$native_american = ifelse(df.pseudo$race == 5, 1, 0)
df.pseudo$other = ifelse(df.pseudo$race == 6, 1, 0)

preds.pseudo <- predict(LA, newdata = df.pseudo, 
                          n.trees = 548, type ="response")
```

#Reference: Gradient boosted Tree model code that we tried.  
```{r}

# #Implementing CV over two parameters: depths and number of trees
# depths <- c(1,2,3,4,5,6,7,8)
# 
# #initialize a list to store models fitted 
# bt.models <- list()
# 
# #set seed to replicate code
# set.seed(123)
# 
# for (i in depths){
#   bt.models[[i]] <- gbm(recidivate ~ ., 
#                         data = df.train, shrinkage = 0.01,
#                         #set "bernoulli" for classification, max number of trees set to 2000 
#                         distribution="bernoulli", n.trees = 5000, 
#                         interaction.depth = i, 
#                         #cv.folds implements cv with 10 k-folds
#                         cv.folds = 10) 
#   print(i)
#   
# }
# 
# bt.one <- bt.models[[4]]

# trees vs. cv error graph ("best model")
# plot(x = 1:bt.one$n.trees == 5000, y = bt.one$cv.error)


#the graph shows that cv.error increases after hitting bottom at
#around 500. 

#Now, we want to consider all of the models: 
#All values of interaction depth AND all number of trees,
#To find the best combination based on these CV results!

#n.depths <- length(bt.models)
#depths <- rep(NA, n.depths)
#min.cv.error <- rep(NA, n.depths)
#best.n.trees <- rep(NA, n.depths)

#Loop over all 8 models to store best values 
# for (i in 1:n.depths){
  
#  bt.curr <- bt.models[[i]]
#  depths[i] <- bt.curr$interaction.depth
#  min.cv.error[i] <- min(bt.curr$cv.error)
#  best.n.trees[i] <- which.min(bt.curr$cv.error)
#  rm(bt.curr)
  
#}

#which model had the lowest cv error 
# m <- which.min(min.cv.error)
# final.ntrees <- best.n.trees[m]
# final.ntrees
# final.depth <- depths[m]
# final.depth

# see which feature has the most predictive power for our "best model"
# summary(bt.models[[m]])

#     trees vs. cv error graph ("best model")

# plot(x = 1:bt.models[[m]]$n.trees, y = bt.models[[m]]$cv.error)

#     depth vs. cv error graph ("best model")

# plot(x = 1:bt.models[[m]]$interaction.depth, y = bt.models[[m]]$cv.error)




# final model 

# test.dat.preds <- predict(bt.models[[m]], newdata = df.test, 
#                          n.trees = final.ntrees, type ="response")
#  threshold <- 0.5
# y.test.hat <- as.numeric(test.dat.preds > threshold)

# Model_8_ACC <- mean(df.test$recidivate == y.test.hat) #accuracy

# Model_8_PRC <- sum(y.test.hat[df.test$recidivate == 1])/sum(y.test.hat) #precision

# Model_8_RC <-sum(y.test.hat[df.test$recidivate == 1])/sum(df.test$recidivate) #recall
